{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_TG56JeAQF7SRuFAdG-HAHT8jkOijvrq",
      "authorship_tag": "ABX9TyNLn0MavaBoxt57WRQhv/1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hugoheldens/ClonacionWeb/blob/main/SiteClonV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import os\n",
        "\n",
        "def clonar_sitio_web(url):\n",
        "    # Obtener el contenido HTML de la página principal\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        html = response.text\n",
        "    else:\n",
        "        print(\"Error al acceder a la URL:\", response.status_code)\n",
        "        return\n",
        "\n",
        "    # Crear un objeto BeautifulSoup para analizar el HTML\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Crear un directorio para almacenar el contenido clonado\n",
        "    directorio_destino = 'sitio_clonado'\n",
        "    if not os.path.exists(directorio_destino):\n",
        "        os.makedirs(directorio_destino)\n",
        "\n",
        "    # Descargar y guardar cada archivo enlazado encontrado en el sitio web\n",
        "    enlaces = soup.find_all('a')\n",
        "    for enlace in enlaces:\n",
        "        href = enlace.get('href')\n",
        "        if href:\n",
        "            # Convertir el enlace relativo a enlace absoluto\n",
        "            enlace_absoluto = urljoin(url, href)\n",
        "\n",
        "            # Descargar el archivo solo si pertenece al mismo dominio\n",
        "            if urlparse(enlace_absoluto).netloc == urlparse(url).netloc:\n",
        "                # Realizar la solicitud HTTP para descargar el archivo\n",
        "                try:\n",
        "                    response = requests.get(enlace_absoluto)\n",
        "                    if response.status_code == 200:\n",
        "                        # Obtener el nombre del archivo del enlace\n",
        "                        nombre_archivo = os.path.basename(urlparse(enlace_absoluto).path)\n",
        "\n",
        "                        # Guardar el archivo en el directorio destino\n",
        "                        ruta_archivo = os.path.join(directorio_destino, nombre_archivo)\n",
        "                        with open(ruta_archivo, 'wb') as archivo:\n",
        "                            archivo.write(response.content)\n",
        "                        print(\"Archivo descargado:\", nombre_archivo)\n",
        "                    else:\n",
        "                        print(\"Error al descargar el archivo:\", enlace_absoluto, response.status_code)\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(\"Error en la solicitud:\", e)\n",
        "\n",
        "    print(\"Clonado completado!\")\n",
        "\n",
        "# URL del sitio web a clonar\n",
        "url_sitio_web = input(\"Introduce la URL del sitio web: \")\n",
        "clonar_sitio_web(url_sitio_web)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zArFTO1lOp-",
        "outputId": "5ff312d4-d96e-4b86-95dc-17b65dc815f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduce la URL del sitio web: http://colegionezahualcoyotl.edu.mx/\n",
            "Archivo descargado: plataforma.html\n",
            "Archivo descargado: plataforma.html\n",
            "Archivo descargado: inscripciones.html\n",
            "Archivo descargado: mapa.html\n",
            "Archivo descargado: modelo.html\n",
            "Archivo descargado: becas.html\n",
            "Archivo descargado: preescolar.html\n",
            "Archivo descargado: primaria.html\n",
            "Archivo descargado: especiales.html\n",
            "Archivo descargado: mapa.html\n",
            "Archivo descargado: aviso-de-privacidad.html\n",
            "Clonado completado!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import os\n",
        "\n",
        "def clonar_sitio_web(url):\n",
        "    # Obtener el contenido HTML de la página principal\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        html = response.text\n",
        "    else:\n",
        "        print(\"Error al acceder a la URL:\", response.status_code)\n",
        "        return\n",
        "\n",
        "    # Crear un objeto BeautifulSoup para analizar el HTML\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Crear un directorio para almacenar el contenido clonado\n",
        "    directorio_destino = 'F:\\\\Users\\\\Jared\\\\ClonacionWeb\\\\temasEspecialesSeguridadInformatica'\n",
        "    if not os.path.exists(directorio_destino):\n",
        "        os.makedirs(directorio_destino)\n",
        "\n",
        "    # Descargar y guardar cada archivo enlazado encontrado en el sitio web\n",
        "    enlaces = soup.find_all(['a', 'link'])\n",
        "    for enlace in enlaces:\n",
        "        href = enlace.get('href')\n",
        "        if href:\n",
        "            # Convertir el enlace relativo a enlace absoluto\n",
        "            enlace_absoluto = urljoin(url, href)\n",
        "\n",
        "            # Descargar el archivo solo si pertenece al mismo dominio\n",
        "            if urlparse(enlace_absoluto).netloc == urlparse(url).netloc:\n",
        "                # Realizar la solicitud HTTP para descargar el archivo\n",
        "                try:\n",
        "                    response = requests.get(enlace_absoluto)\n",
        "                    if response.status_code == 200:\n",
        "                        # Obtener el nombre del archivo del enlace\n",
        "                        nombre_archivo = os.path.basename(urlparse(enlace_absoluto).path)\n",
        "\n",
        "                        # Guardar el archivo en el directorio destino\n",
        "                        ruta_archivo = os.path.join(directorio_destino, nombre_archivo)\n",
        "                        with open(ruta_archivo, 'wb') as archivo:\n",
        "                            archivo.write(response.content)\n",
        "                        print(\"Archivo descargado:\", nombre_archivo)\n",
        "                    else:\n",
        "                        print(\"Error al descargar el archivo:\", enlace_absoluto, response.status_code)\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(\"Error en la solicitud:\", e)\n",
        "\n",
        "    print(\"Clonado completado!\")\n",
        "\n",
        "# URL del sitio web a clonar\n",
        "url_sitio_web = input(\"Introduce la URL del sitio web: \")\n",
        "clonar_sitio_web(url_sitio_web)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IsH6RRLoXka",
        "outputId": "4d4b1f05-4365-47ed-8097-ceb291005bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduce la URL del sitio web: http://colegionezahualcoyotl.edu.mx/\n",
            "Archivo descargado: all.css\n",
            "Archivo descargado: plataforma.html\n",
            "Archivo descargado: plataforma.html\n",
            "Archivo descargado: inscripciones.html\n",
            "Archivo descargado: mapa.html\n",
            "Archivo descargado: modelo.html\n",
            "Archivo descargado: becas.html\n",
            "Archivo descargado: preescolar.html\n",
            "Archivo descargado: primaria.html\n",
            "Archivo descargado: especiales.html\n",
            "Archivo descargado: mapa.html\n",
            "Archivo descargado: aviso-de-privacidad.html\n",
            "Clonado completado!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import os\n",
        "\n",
        "def clonar_sitio_web(url):\n",
        "    # Obtener el contenido HTML de la página principal\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        html = response.text\n",
        "    else:\n",
        "        print(\"Error al acceder a la URL:\", response.status_code)\n",
        "        return\n",
        "\n",
        "    # Crear un objeto BeautifulSoup para analizar el HTML\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Crear un directorio para almacenar el contenido clonado\n",
        "    directorio_destino = 'sitio_clonado'\n",
        "    if not os.path.exists(directorio_destino):\n",
        "        os.makedirs(directorio_destino)\n",
        "\n",
        "    # Descargar y guardar cada archivo enlazado encontrado en el sitio web\n",
        "    enlaces = soup.find_all(['a', 'link', 'script'])\n",
        "    for enlace in enlaces:\n",
        "        href = enlace.get('href')\n",
        "        src = enlace.get('src')\n",
        "        if href:\n",
        "            # Convertir el enlace relativo a enlace absoluto\n",
        "            enlace_absoluto = urljoin(url, href)\n",
        "\n",
        "            # Descargar el archivo solo si pertenece al mismo dominio\n",
        "            if urlparse(enlace_absoluto).netloc == urlparse(url).netloc:\n",
        "                descargar_archivo(enlace_absoluto, directorio_destino)\n",
        "\n",
        "        if src:\n",
        "            # Convertir la ruta relativa a ruta absoluta\n",
        "            src_absoluto = urljoin(url, src)\n",
        "\n",
        "            # Descargar el archivo solo si pertenece al mismo dominio\n",
        "            if urlparse(src_absoluto).netloc == urlparse(url).netloc:\n",
        "                descargar_archivo(src_absoluto, directorio_destino)\n",
        "\n",
        "    print(\"Clonado completado!\")\n",
        "\n",
        "def descargar_archivo(url, directorio_destino):\n",
        "    # Realizar la solicitud HTTP para descargar el archivo\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            # Obtener el nombre del archivo de la URL\n",
        "            nombre_archivo = os.path.basename(urlparse(url).path)\n",
        "\n",
        "            # Guardar el archivo en el directorio destino\n",
        "            ruta_archivo = os.path.join(directorio_destino, nombre_archivo)\n",
        "            with open(ruta_archivo, 'wb') as archivo:\n",
        "                archivo.write(response.content)\n",
        "            print(\"Archivo descargado:\", nombre_archivo)\n",
        "        else:\n",
        "            print(\"Error al descargar el archivo:\", url, response.status_code)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(\"Error en la solicitud:\", e)\n",
        "\n",
        "# URL del sitio web a clonar\n",
        "url_sitio_web = input(\"Introduce la URL del sitio web: \")\n",
        "clonar_sitio_web(url_sitio_web)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2j2DlCQp4yQ",
        "outputId": "3d81fdfd-7e60-4691-c221-8c2b6931f9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduce la URL del sitio web: http://colegionezahualcoyotl.edu.mx/\n",
            "Archivo descargado: all.css\n",
            "Archivo descargado: plataforma.html\n",
            "Archivo descargado: plataforma.html\n",
            "Archivo descargado: inscripciones.html\n",
            "Archivo descargado: mapa.html\n",
            "Archivo descargado: modelo.html\n",
            "Archivo descargado: becas.html\n",
            "Archivo descargado: preescolar.html\n",
            "Archivo descargado: primaria.html\n",
            "Archivo descargado: especiales.html\n",
            "Archivo descargado: mapa.html\n",
            "Archivo descargado: aviso-de-privacidad.html\n",
            "Archivo descargado: scroll-entrance.js\n",
            "Clonado completado!\n"
          ]
        }
      ]
    }
  ]
}